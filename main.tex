\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{IFT6285 TP}
\author{Jeanne, Esdras and Jason WU}

\begin{document}
\maketitle

\begin{abstract}
Dans ce travail pratique,  on discutera les methodes de la prédiction d'une séquence de formes à partir d'une séquence de lemmes.
\end{abstract}

\section{Introduction}
Quel sont les lemmas?

\section{Analyse des données}

\subsection{La relation parmi les train, test et validation données}

\subsection{La statistique}

\subsection{Réduire les bruits}

\subsection{Baseline}
Si on faisait rien, combien de précision de prédiction a-t-on gagné?

\section{Outil}
\subsection{Traitement}
\subsubsection{Comparison parmi NLTK, TextBlob, Stanford CoreNLP, SpaCy and Gensim}
\begin{itemize}
\item Modèles entraîné ou pas?
\item La vitesse
\item NLTK vs. spaCy \cite{noauthor_nltk_2016} Selon https://blog.thedataincubator.com/2016/04/nltk-vs-spacy-natural-language-processing-in-python/, spaCy est 20 fois plus vite que NLTK sur le travail de POS.
\end{itemize}
\subsubsection{NLTK}
\begin{enumerate}
\item Il faut considérer que les donnée on va nourrir au NLTK est dans quelle forme. En fait, nous sommes distribué les donnés dans la paire de frome-lemma.
\end{enumerate}

\subsection{Evaluation}
\begin{itemize}
\item Shell
\item Python
\item Excel
\end{itemize}

\section{Fondations de l’Algorithme}
\subsection{Structure de phrase et Parseur }
\subsection{Probabilité Modèle}
\subsubsection{Naïve Bayes}
$ P(A \mid B) = \frac{P(B \mid A) \, P(A)}{P(B)} $

{ Nos variables aléatoires sont discrètes. -> MultiNomial Naive Bayes -> Smoothing}

\subsubsection{Modèles de Markov cachée HMM}
\begin{itemize}
\item Théorie
\item Lier les séquences aux états
\item Lissage
\item Apprentissage des paramètres
\end{itemize}


\subsection{Générateur vs Discriminant}

\subsection{Local Maximum}

\subsection{Label bias problem}

\section{Related work}
\section{Discussion}
\section{Résumé}
\cite{greenwade93}


\bibliographystyle{alpha}
\bibliography{bib_jason,sample}
\end{document}